import os
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞ tokenizer
tokenizer = AutoTokenizer.from_pretrained("microsoft/DialoGPT-medium")
model = AutoModelForCausalLM.from_pretrained("microsoft/DialoGPT-medium")
tokenizer.pad_token = tokenizer.eos_token
def read_all_text_files(folder="."):
    content_list = []
    allowed_extensions = [".txt", ".py", ".md", ".json", ".csv"]

    for filename in os.listdir(folder):
        file_path = os.path.join(folder, filename)
        if os.path.isfile(file_path) and any(filename.endswith(ext) for ext in allowed_extensions):
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    content = f.read()
                    content_list.append(f"--- FILE: {filename} ---\n{content[:1000]}")  # ‡∏ï‡∏±‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏•‡∏∞ 1000 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£
            except Exception as e:
                print(f"Error reading file {filename}: {e}")

    return "\n\n".join(content_list)

def chat_with_bot(chat_history_ids, user_input):
    encoded_dict = tokenizer.encode_plus(
        user_input + tokenizer.eos_token,
        return_tensors='pt',
        padding=True,
        truncation=True,
    )
    input_ids = encoded_dict['input_ids']
    attention_mask = encoded_dict['attention_mask']

    if chat_history_ids is not None:
        input_ids = torch.cat([chat_history_ids, input_ids], dim=-1)
        attention_mask = torch.ones_like(input_ids)  # ‡πÑ‡∏°‡πà‡∏°‡∏µ padding ‡∏à‡∏£‡∏¥‡∏á ‡∏à‡∏∂‡∏á mask ‡πÄ‡∏õ‡πá‡∏ô 1

    chat_history_ids = model.generate(
        input_ids,
        attention_mask=attention_mask,
        max_length=1000,
        pad_token_id=tokenizer.eos_token_id,
        no_repeat_ngram_size=3,
        do_sample=True,
        top_k=50,
        top_p=0.95,
    )

    response = tokenizer.decode(chat_history_ids[:, input_ids.shape[-1]:][0], skip_special_tokens=True)
    return response, chat_history_ids

def main():
    print("üí¨ ‡∏û‡∏¥‡∏°‡∏û‡πå 'read data' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ AI ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå")
    print("üí¨ ‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏∑‡πà‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏ä‡∏ï‡∏Å‡∏±‡∏ö AI")
    print("üí¨ ‡∏û‡∏¥‡∏°‡∏û‡πå 'exit' ‡∏´‡∏£‡∏∑‡∏≠ 'quit' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡∏≠‡∏Å\n")

    chat_history_ids = None
    folder_data = ""

    while True:
        user_input = input("You: ").strip()
        if user_input.lower() in ['exit', 'quit']:
            print("üëã ‡∏ö‡πä‡∏≤‡∏¢‡∏ö‡∏≤‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö!")
            break

        if user_input.lower() == "read data":
            print("üìÅ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå...")
            folder_data = read_all_text_files()
            if not folder_data:
                print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ô‡∏µ‡πâ")
            else:
                prompt = f"‡∏ä‡πà‡∏ß‡∏¢‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ‡πÉ‡∏´‡πâ‡∏´‡∏ô‡πà‡∏≠‡∏¢:\n{folder_data[:2000]}"  # ‡∏ï‡∏±‡∏î‡πÉ‡∏´‡πâ‡∏™‡∏±‡πâ‡∏ô‡∏û‡∏≠‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì
                chat_history_ids = None  # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÅ‡∏ä‡∏ï‡πÉ‡∏´‡∏°‡πà
                response, chat_history_ids = chat_with_bot(chat_history_ids, prompt)
                print("AI:", response, "\n")

        else:
            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ hi ‡πÉ‡∏´‡πâ‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï‡πÅ‡∏ä‡∏ï‡πÅ‡∏•‡∏∞‡∏ó‡∏±‡∏Å‡∏ó‡∏≤‡∏¢
            if user_input.lower() == "hi":
                chat_history_ids = None
                response, chat_history_ids = chat_with_bot(chat_history_ids, "Hi, I am your assistant. How can I help you?")
                print("AI:", response, "\n")
            else:
                # ‡πÅ‡∏ä‡∏ï‡∏ï‡πà‡∏≠‡∏à‡∏≤‡∏Å‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡πÄ‡∏î‡∏¥‡∏°
                response, chat_history_ids = chat_with_bot(chat_history_ids, user_input)
                print("AI:", response, "\n")

if __name__ == "__main__":
    main()

